# Module 3: RAG and LLM Evaluation - Understanding Search & Answer Quality

This module delves into the critical aspect of evaluating RAG (Retrieval Augmented Generation) and Large Language Models (LLMs). Effective evaluation is essential for building robust and performant RAG systems, helping us understand how well our retrieval mechanisms find relevant information and how accurately and coherently our LLMs generate answers.

## Key Evaluation Areas Covered

### 1. Retrieval Evaluation

Retrieval evaluation focuses on assessing the effectiveness of the search component in finding relevant documents based on a given query. A well-performing retrieval system is the foundation of a good RAG system.

#### a. Data Preparation for Evaluation
- **Ground Truth / Gold Standard Data**: Creating reliable datasets that map queries to relevant documents (document IDs). This often involves manual annotation or leveraging LLMs to generate high-quality question-answer pairs and identify verifiable sources.
- **Documents with IDs**: Ensuring that the documents in your knowledge base have unique identifiers for matching against ground truth.

#### b. Ranking Evaluation Metrics
To measure how well search results are ordered and if the correct documents are returned, we use:
- **Hit Rate**: The proportion of queries for which the relevant document is found within the top-K retrieved results.
- **Mean Reciprocal Rank (MRR)**: Measures the average of the reciprocal ranks of the first relevant document. A higher MRR means relevant documents are found higher in the search results.

#### c. Search Approaches and Evaluation
We explore and evaluate different search methodologies:
- **Minsearch (Text Search)**: A basic text search. We learn to configure it with parameters like `text_fields` (for indexing), `keyword_fields` (for exact filtering), and `boost_dict` (to prioritize certain fields in scoring).
- **Minsearch (Vector Search)**: Leveraging embeddings for semantic search.
    - **Embeddings**: Creating numerical representations (vectors) of text using techniques like TF-IDF and Singular Value Decomposition (SVD).
    - **Variations**: Experimenting with different text inputs for embedding documents (e.g., just the "question" field vs. combining "question" and "text" fields) to understand their impact on search relevance.
- **Qdrant (Vector Database)**: Using a dedicated vector database for high-performance vector search. This involves:
    - **Embedding Models**: Integrating pre-trained models (e.g., Jina embeddings via `fastembed` or `transformers` library) to convert text into vectors.
    - **Indexing**: Storing document embeddings along with their metadata (payload) in Qdrant collections.
    - **Filtering**: Applying metadata filters (e.g., by 'course' category) during vector search to narrow down results.
    - **Qdrant Point IDs**: Understanding the requirements for unique point identifiers (UUIDs or unsigned integers) for proper indexing.

### 2. RAG Answer Quality Evaluation

Beyond just retrieval, it's crucial to assess the quality of the answers generated by the LLM, especially when augmented by retrieved documents. This module focuses on offline evaluation techniques by comparing system-generated answers against reference answers.

#### a. Offline vs. Online Evaluation
- **Offline Evaluation**: Assessing system performance using predefined datasets without live user interaction. This is typically done during development and testing.
- **Online Evaluation**: Measuring performance in a live environment with real user feedback (e.g., A/B testing, user ratings). This module focuses primarily on offline methods.

#### b. Answer Similarity Metrics
We explore quantifiable metrics to compare generated answers with ground truth answers:
- **Cosine Similarity**: A measure of similarity between two non-zero vectors that use the cosine of the angle between them. In the context of RAG, we use it to compare the semantic similarity between an LLM's generated answer and an original, reference answer by encoding them into vector space.
    - **A->Q->A' (Answer to Question to Answer Prime) Cosine Similarity**: A specific evaluation technique where we assess the semantic alignment between an LLM's answer and a known good answer.
- **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**: A set of metrics commonly used for summarization and machine translation tasks. ROUGE compares an automatically produced summary or translation against a set of reference summaries or translations.
    - **N-gram Overlap**: Measures the overlap of unigrams (ROUGE-1), bigrams (ROUGE-2), or longer sequences between the generated and reference texts.
    - **Longest Common Subsequence (ROUGE-L)**: Focuses on the longest sequence of words (not necessarily consecutive) that is common to both texts, providing a measure of fluency and structural similarity.
    - **Precision, Recall, F1-score**: ROUGE metrics typically provide these for each type of overlap, with F1 being a balanced measure of precision and recall.

### 3. Practical Considerations

- **Pipeline Management**: Utilizing `sklearn.pipeline.make_pipeline` to streamline the sequence of data transformations (e.g., TF-IDF Vectorization followed by SVD).
- **Dependency Management**: Understanding the roles of libraries like `minsearch`, `qdrant-client`, `fastembed`, `scikit-learn`, `rouge`, and `numpy` in building and evaluating RAG systems.

This module equips learners with foundational knowledge and practical skills to objectively assess the performance of their retrieval components and the quality of their LLM-generated answers within a RAG framework.